{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from transformers import pipeline\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import librosa\n",
    "import time\n",
    "import os\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf \n",
    "import pyloudnorm as pyln \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_huggingface_model(model_name):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained model from Hugging Face's Transformers library for automatic speech recognition (ASR).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str\n",
    "        The name or identifier of the pre-trained model available on Hugging Face's model hub.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    trained_model : Hugging Face pipeline or None\n",
    "        Returns the loaded ASR model pipeline if successful. If an error occurs during model loading, \n",
    "        returns None and prints the error message.\n",
    "\n",
    "    Raises:\n",
    "    -------\n",
    "    Prints an error message if the model fails to load due to any exception.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> model = load_huggingface_model(\"facebook/wav2vec2-large-960h\")\n",
    "    >>> if model:\n",
    "    >>>     result = model(\"Audio file path or data\")\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Loading Model from Huggingface\")\n",
    "        # loading the model from huggingface\n",
    "        trained_model = pipeline(\"automatic-speech-recognition\",model_name)\n",
    "        print(\"Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load the model. Following error occured:\\n{e}\")\n",
    "        return None\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_local_model(model_path, processor_path, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained Wav2Vec2 model and processor from local directories for automatic speech recognition (ASR).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_path : str\n",
    "        The file path to the pre-trained Wav2Vec2 model directory.\n",
    "    \n",
    "    processor_path : str\n",
    "        The file path to the Wav2Vec2 processor directory, used for tokenization and feature extraction.\n",
    "\n",
    "    device : str, optional (default=\"cpu\")\n",
    "        The device on which to load the model, either 'cpu' or 'cuda' for GPU.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model : Wav2Vec2ForCTC\n",
    "        The loaded Wav2Vec2 model for ASR.\n",
    "\n",
    "    processor : Wav2Vec2Processor\n",
    "        The processor corresponding to the loaded model, used for feature extraction.\n",
    "\n",
    "    Raises:\n",
    "    -------\n",
    "    Prints an error message if the model or processor fails to load due to any exception.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> model, processor = load_local_model(\"/path/to/model\", \"/path/to/processor\", device=\"cuda\")\n",
    "    >>> if model and processor:\n",
    "    >>>     # Use model and processor for speech recognition tasks\n",
    "    >>>     input_values = processor(\"Audio file path\", return_tensors=\"pt\").input_values\n",
    "    >>>     logits = model(input_values.to(\"cuda\")).logits\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Loading Local Model\")\n",
    "        # loading the model from local directory\n",
    "        model = Wav2Vec2ForCTC.from_pretrained(model_path).to(device)\n",
    "        processor = Wav2Vec2Processor.from_pretrained(processor_path)\n",
    "        print(\"Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load the model. Following error occured:\\n{e}\")\n",
    "        return None\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def getFileNameAndExtension(file_path):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    -------------\n",
    "    This function takes a file path as input and it will return the filename and extension for that file path\n",
    "    \n",
    "    Arguments:\n",
    "    -------------\n",
    "    file_path: Absolute or relative path for the input file\n",
    "    \n",
    "    Returns:\n",
    "    -------------\n",
    "    filename: name of the file\n",
    "    extension: extension of file\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> getFileNameAndExtension(\"/path/to/file.txt\")\n",
    "    ('file', 'txt')\n",
    "    \"\"\"\n",
    "    # get the basename of the file from its absolute path\n",
    "    basename = os.path.basename(file_path)\n",
    "    # get the filename and file extension\n",
    "    filename, ext = basename.split(\".\")\n",
    "    return filename,ext\n",
    "\n",
    "def segmentLargeArrayforHuggingface(inputArray,array_length,chunksize=200000):\n",
    "    \"\"\"\n",
    "    Segments a large input array into smaller chunks based on the specified chunk size.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    inputArray : list or numpy.ndarray\n",
    "        The large input array to be segmented.\n",
    "    \n",
    "    array_length : int\n",
    "        The length of the input array. This should match `len(inputArray)` for correct segmentation.\n",
    "    \n",
    "    chunksize : int, optional (default=200000)\n",
    "        The size of each segment. If the array length is not perfectly divisible by `chunksize`,\n",
    "        the last segment may be smaller than `chunksize`.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list_of_segments : list of lists or list of numpy.ndarray\n",
    "        A list containing the segmented parts of the original array. Each segment is a portion of \n",
    "        the input array with a length of up to `chunksize`.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> input_array = [i for i in range(600000)]\n",
    "    >>> segments = segmentLargeArrayforHuggingface(input_array, len(input_array), chunksize=200000)\n",
    "    >>> len(segments)\n",
    "    3  # Three segments with up to 200000 elements each.\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - This function slices the input array based on the `chunksize` and operates over the first dimension.\n",
    "    - If `array_length` is smaller than `chunksize`, the entire array is returned as a single segment.\n",
    "    \"\"\"\n",
    "    list_of_segments = []\n",
    "    for i in range(0,array_length+1,chunksize):\n",
    "        list_of_segments.append(inputArray[i:i+chunksize])\n",
    "    return list_of_segments \n",
    "\n",
    "def segmentLargeArrayForLocal(inputTensor,chunksize=200000):\n",
    "    \"\"\"\n",
    "    Segments a large input tensor into smaller chunks based on the specified chunk size.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    inputTensor : torch.Tensor or numpy.ndarray\n",
    "        The input 2D tensor or array to be segmented. The function assumes that the tensor has at least 2 dimensions, \n",
    "        where the second dimension is segmented.\n",
    "    \n",
    "    chunksize : int, optional (default=200000)\n",
    "        The size of each segment along the second dimension. If the tensor length is not perfectly divisible by the \n",
    "        chunk size, the last segment may be smaller.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list_of_segments : list of torch.Tensor or numpy.ndarray\n",
    "        A list containing the segmented tensors or arrays. Each segment is a portion of the original input \n",
    "        tensor along its second dimension.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> input_tensor = torch.randn(3, 600000)\n",
    "    >>> segments = segmentLargeArrayForLocal(input_tensor, chunksize=200000)\n",
    "    >>> len(segments)\n",
    "    3  # Three segments with 200000 elements each along the second dimension.\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - This function operates along the second dimension of a 2D tensor or array.\n",
    "    - If `inputTensor.shape[1]` is smaller than `chunksize`, the entire tensor is returned as a single segment.\n",
    "    \"\"\"\n",
    "    # print(inputTensor)\n",
    "    list_of_segments = []\n",
    "    tensor_length = inputTensor.shape[1]\n",
    "    for i in range(0,tensor_length+1,chunksize):\n",
    "        list_of_segments.append(inputTensor[:,i:i+chunksize])\n",
    "    return list_of_segments \n",
    "\n",
    "def adjust_volume(data,sr=16000,norm=\"peak\"):\n",
    "    \"\"\"\n",
    "    Adjusts the volume of the audio data using either peak normalization or loudness normalization.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : numpy.ndarray\n",
    "        The input audio data, typically a 2D array where the second dimension represents the audio samples.\n",
    "    \n",
    "    sr : int, optional (default=16000)\n",
    "        The sample rate of the audio data, used for the loudness measurement. Defaults to 16,000 Hz.\n",
    "    \n",
    "    norm : str, optional (default=\"peak\")\n",
    "        The type of normalization to apply:\n",
    "        - \"peak\": Peak normalization adjusts the volume so that the loudest peak reaches -1 dB.\n",
    "        - \"fixed\": Loudness normalization adjusts the volume to a fixed loudness level (0 dB).\n",
    "        - Any other value will leave the audio unchanged.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    peak_normalized_audio : numpy.ndarray\n",
    "        The normalized audio data after applying the specified normalization technique.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> normalized_audio = adjust_volume(audio_data, sr=16000, norm=\"peak\")\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - Peak normalization adjusts the volume relative to the highest peak in the audio file.\n",
    "    - Loudness normalization adjusts the overall perceived loudness to a fixed level (0 dB by default).\n",
    "    - The `pyln.Meter` object is used to measure loudness according to ITU BS.1770 standards.\n",
    "    \"\"\"\n",
    "    # Peak normalization of all audio to -1dB\n",
    "    meter = pyln.Meter(sr) #create BS.1770 Meter\n",
    "    # print(data)\n",
    "    # print(np.transpose(data).shape)\n",
    "    loudness = meter.integrated_loudness(np.transpose(data)) \n",
    "    # print(f'Before: {loudness} dB')\n",
    "    if norm == \"peak\":\n",
    "        # This is peak normalization which depends on the original volume of audio file\n",
    "        peak_normalized_audio = pyln.normalize.peak(data,-1.0)\n",
    "    elif norm==\"fixed\":\n",
    "        # Actually this is loudness normalization to a fixed level irrespective of volume in original file\n",
    "        peak_normalized_audio = pyln.normalize.loudness(data, loudness, 0)\n",
    "    else:\n",
    "        peak_normalized_audio = data\n",
    "    loudness = meter.integrated_loudness(np.transpose(peak_normalized_audio)) \n",
    "    # print(f'After peak normalization: {loudness} dB')\n",
    "    return peak_normalized_audio\n",
    "\n",
    "def convertAudio(src_audio_path,format=\"mp3\"):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    -------------\n",
    "    This function converts audio files from m4a files to mp3 files(by default) or any other file format specified by the user\n",
    "    \n",
    "    Arguments:\n",
    "    -------------\n",
    "    src_audio_path: path of the audio file in m4a format\n",
    "    \n",
    "    Returns:\n",
    "    -------------\n",
    "    dest_audio_path: path of the audio file in desired format\n",
    "    \"\"\"\n",
    "    filename, ext = getFileNameAndExtension(src_audio_path)\n",
    "    # if the extension is already flac no need to convert\n",
    "    if ext == format:\n",
    "        return src_audio_path\n",
    "    # create a temporary flac file as flac is supported in torchaudio\n",
    "    dest_audio_path = f\"temp/{filename}.{format}\"  \n",
    "    # using AudioSegment from pydub to convert from any audio format to flac as flac is compressed format of wav and torchaudio only supports wav and flac\n",
    "    audio = AudioSegment.from_file(src_audio_path,format=\"m4a\")\n",
    "    # Export the audio to flac file\n",
    "    audio.export(dest_audio_path, format=format)\n",
    "    return dest_audio_path\n",
    "\n",
    "def deleteTempAudio(dest_audio_path):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    -------------\n",
    "    This function deletes the temporary converted audio files\n",
    "    \n",
    "    Arguments:\n",
    "    -------------\n",
    "    dest_audio_path: path of the audio file in mp3 format created temporarily\n",
    "    \n",
    "    Returns:\n",
    "    -------------\n",
    "    None\n",
    "    \"\"\"\n",
    "    os.remove(dest_audio_path)\n",
    "\n",
    "def writeOutputToFile(output, audio_file_path,translated=False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    -------------\n",
    "    This function writes the generated Nepali transcript to a file inside the output folder\n",
    "    \n",
    "    Arguments:\n",
    "    -------------\n",
    "    output: Nepali text transcript generated by ASR model\n",
    "    audio_file_path: File path of the input audio\n",
    "    \n",
    "    Returns:\n",
    "    -------------\n",
    "    destination_file_path: File path of the text transcript\n",
    "    \"\"\"\n",
    "    filename,ext = getFileNameAndExtension(audio_file_path)\n",
    "\n",
    "    destination_file_path = f\"./transcripts/{filename}.txt\"\n",
    "    with open(destination_file_path,\"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(output)\n",
    "    return destination_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTranscriptFromHuggingFaceModel(audio_input,model):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    -------------\n",
    "    This function generates Nepali transcript for the Nepali speech input\n",
    "    \n",
    "    Arguments:\n",
    "    -------------\n",
    "    audio_input: Path of audio file for which transcript is generated\n",
    "    model: pretrained ASR model to perform speech recognition\n",
    "    \n",
    "    Returns:\n",
    "    -------------\n",
    "    output: Nepali text transcript for the given audio input\n",
    "    \"\"\"\n",
    "    # set mono=True as the SpeechRecognitionPipelince can only work with mono audio, Also sampling rate is set to 16k as the model was trained at this sampling rate\n",
    "    speech_array, sr = librosa.load(audio_input,mono=True,sr=16000)\n",
    "    array_length = speech_array.shape[0]\n",
    "    # print(speech_array, array_length)\n",
    "    # for longer audio, segmentation needs to be done to prevent program from consuming entire RAM which may cause error, so I am diving the entire audio to smaller segments and will process these segments\n",
    "    if array_length > 250000:\n",
    "        list_of_segments = segmentLargeArrayforHuggingface(speech_array,array_length, 200000)\n",
    "        # print(list_of_segments)\n",
    "        output = ''\n",
    "        for segment in list_of_segments:\n",
    "            output += model(segment)[\"text\"]\n",
    "    else:\n",
    "        output = model(audio_input)[\"text\"]\n",
    "    return output\n",
    "\n",
    "\n",
    "def generateTranscriptFromLocalModel(input_file,model, processor, device=\"cpu\", do_segment=True):\n",
    "    \"\"\"\n",
    "    Generates a transcript from an audio file using a pre-trained local speech recognition model.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file : str\n",
    "        The file path to the input audio file to be transcribed.\n",
    "    \n",
    "    model : torch.nn.Module\n",
    "        The pre-trained speech recognition model (e.g., Wav2Vec2ForCTC) used for generating predictions.\n",
    "    \n",
    "    processor : Wav2Vec2Processor\n",
    "        The processor used to prepare input data and decode model outputs, including tokenization and feature extraction.\n",
    "    \n",
    "    device : str, optional (default=\"cpu\")\n",
    "        The device on which to perform inference, such as \"cpu\" or \"cuda\" for GPU inference.\n",
    "    \n",
    "    do_segment : bool, optional (default=True)\n",
    "        If True, the function will segment large audio files (greater than 10 seconds) into smaller chunks for processing.\n",
    "        If False, the entire audio file will be processed in one go, irrespective of its length.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    output : str\n",
    "        The generated transcript of the input audio file.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> transcript = generateTranscriptFromLocalModel(\"audio.wav\", model, processor, device=\"cuda\", do_segment=True)\n",
    "    >>> print(transcript)\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The function first loads the audio file and resamples it to 16,000 Hz for model compatibility.\n",
    "    - Audio longer than 10 seconds is segmented into smaller chunks of 200,000 samples if `do_segment=True`.\n",
    "    - The function normalizes the audio using fixed loudness normalization and converts it to a format suitable \n",
    "      for the model.\n",
    "    - For each segment or the full audio, logits are obtained from the model and decoded into a transcript.\n",
    "\n",
    "    \"\"\"\n",
    "    speech_array, sampling_rate = torchaudio.load(input_file)  \n",
    "    speech_array = speech_array.numpy()\n",
    "    speech_array = adjust_volume(speech_array,sampling_rate,norm=\"fixed\") \n",
    "    speech_array = torch.from_numpy(speech_array) \n",
    "    resampler = torchaudio.transforms.Resample(sampling_rate, 16000)    \n",
    "    resampled_array = resampler(speech_array).squeeze()\n",
    "   \n",
    "    if len(resampled_array.shape) == 1:\n",
    "        resampled_array = resampled_array.reshape([1,resampled_array.shape[0]])\n",
    "    # print(resampled_array.shape[1])\n",
    "    if resampled_array.shape[1] >= 200000 and do_segment == True:\n",
    "        print('The input file is longer than 10 seconds')\n",
    "        list_of_segments = segmentLargeArrayForLocal(resampled_array)\n",
    "        # print(list_of_segments)\n",
    "        output = ''\n",
    "        for segment in list_of_segments:\n",
    "            if segment.size()[1] > 0:\n",
    "                logits = model(segment.to(device)).logits\n",
    "                # print(logits)\n",
    "                pred_ids = torch.argmax(logits,dim=-1)[0]\n",
    "                output += processor.decode(pred_ids)\n",
    "            else:\n",
    "                output += ''\n",
    "    else:\n",
    "        print('The input file is less than 10 seconds')\n",
    "        logits = model(resampled_array.to(device)).logits\n",
    "        # print(logits)\n",
    "        pred_ids = torch.argmax(logits, dim = -1)[0]\n",
    "        # print(\"Prediction:\")\n",
    "        output = processor.decode(pred_ids)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTranscriptForFile(input_file_path, hf_model, local_model, local_processor, model_type=\"huggingface\"):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    -------------\n",
    "    This function generates transcript for a single audio file\n",
    "    \n",
    "    Arguments:\n",
    "    -------------\n",
    "    input_file_path: Path to the input audio file\n",
    "    model: Pretrained ASR model to generate transcript\n",
    "    \n",
    "    Returns:\n",
    "    -------------\n",
    "    None\n",
    "    \"\"\"\n",
    "    audio_extensions = ['mp3','wav','flac','m4a']\n",
    "    filename, ext = getFileNameAndExtension(input_file_path)\n",
    "    if ext in audio_extensions:\n",
    "        print(f\"{input_file_path} is a valid audio file, so proceeding to generate transcript\")\n",
    "        if ext == \"m4a\":\n",
    "            print(f\"{filename} is in m4a format, so converting it to mp3 format\")\n",
    "            input_file_path = convertAudio(input_file_path,\"mp3\")\n",
    "        start_time = time.time()\n",
    "        if model_type == \"huggingface\":\n",
    "            output = generateTranscriptFromHuggingFaceModel(input_file_path,hf_model)\n",
    "            print(f\"Transcript generated in {time.time() - start_time} seconds for {filename}\")\n",
    "        elif model_type == \"local\":\n",
    "            output = generateTranscriptFromLocalModel(input_file_path,local_model,local_processor)\n",
    "            print(f\"Transcript generated in {time.time() - start_time} seconds for {filename}\")\n",
    "        if ext == \"m4a\":\n",
    "            print(f\"Deleting temporarily created mp3 file\")\n",
    "            deleteTempAudio(input_file_path)\n",
    "        destination_file_path = writeOutputToFile(output,input_file_path)\n",
    "        print(f\"Transcript for {filename} is written at {destination_file_path}\")\n",
    "    else:\n",
    "        print(f\"{input_file_path} is not a valid audio file, please enter a valid audio file with extension mp3, wav or flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model from Huggingface\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at anish-shilpakar/wav2vec2-nepali were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at anish-shilpakar/wav2vec2-nepali and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Loading Local Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at ./model and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "./input/anushasan.m4a is a valid audio file, so proceeding to generate transcript\n",
      "anushasan is in m4a format, so converting it to mp3 format\n",
      "Transcript generated in 25.77505373954773 seconds for anushasan\n",
      "Deleting temporarily created mp3 file\n",
      "Transcript for anushasan is written at ./transcripts/anushasan.txt\n",
      "Time for huggingface model: 29.825913190841675 seconds\n",
      "./input/anushasan.m4a is a valid audio file, so proceeding to generate transcript\n",
      "anushasan is in m4a format, so converting it to mp3 format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\anaconda3\\envs\\audio2text_env\\lib\\site-packages\\pyloudnorm\\normalize.py:62: UserWarning: Possible clipped samples in output.\n",
      "  warnings.warn(\"Possible clipped samples in output.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input file is longer than 10 seconds\n",
      "Transcript generated in 258.2786259651184 seconds for anushasan\n",
      "Deleting temporarily created mp3 file\n",
      "Transcript for anushasan is written at ./transcripts/anushasan.txt\n",
      "Time for local model: 260.0946364402771 seconds\n"
     ]
    }
   ],
   "source": [
    "hf_model = load_huggingface_model(\"anish-shilpakar/wav2vec2-nepali\")\n",
    "l_model, l_processor =  load_local_model(\"./model\", \"./processor\")\n",
    "\n",
    "t1 = time.time()\n",
    "generateTranscriptForFile('./input/anushasan.m4a',hf_model, l_model, l_processor, model_type=\"huggingface\")\n",
    "t2 = time.time()\n",
    "print(f\"Time for huggingface model: {t2-t1} seconds\")\n",
    "\n",
    "t3 = time.time()\n",
    "generateTranscriptForFile('./input/anushasan.m4a',hf_model, l_model, l_processor, model_type=\"local\")\n",
    "t4 = time.time()\n",
    "print(f\"Time for local model: {t4-t3} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 - Abstractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def load_summarization_model(model_name):\n",
    "    # try:\n",
    "        tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    # except Exception as e:\n",
    "        # print(f\"Failed to load the model. Following error occured:\\n{e}\")\n",
    "        # return None,None\n",
    "        return tokenizer, model\n",
    "\n",
    "model_name = \"Anjaan-Khadka/Nepali-Summarization\"\n",
    "tokenizer, model = load_summarization_model(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = \" तीन नगरपालिकालाई समेटेर भेरी किनारमा बन्न थालेको आधुनिक नमुना सहरको काम तीव्र गतिमा अघि बढेको छ । भेरीगंगा, गुर्भाकोट र लेकबेंसी नगरपालिकामा बन्न थालेको भेरीगंगा उपत्यका नमुना आधुनिक सहर निर्माण हुन लागेको हो । यसले नदी वारि र पारिको ४ सय ६० वर्ग किलोमिटर क्षेत्रलाई समेट्नेछ ।\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstractive_summary(tokenizer, model, article_text):\n",
    "    input_ids = tokenizer(\n",
    "        (article_text),\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=84,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_beams=4\n",
    "    )[0]\n",
    "\n",
    "    summary = tokenizer.decode(\n",
    "        output_ids,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'भारतको भेरीगंगा उपत्यकामा नयाँ सहर निर्माणको काम सुरु भएको छ। कमल परियारअनि यो पनि'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstractive_summary(tokenizer, model, article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-Rank - Extractive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = open(\"./text_rank/stopwords.txt\",'r',encoding=\"utf-8\").read()\n",
    "word_endings = open(\"./text_rank/word_endings.txt\",'r',encoding='utf-8').read() \n",
    "kriyapads = open(\"./text_rank/minimal_kriyapad.txt\",'r',encoding=\"utf-8\").read().split(\"\\n\")\n",
    "samyojaks = open(\"./text_rank/samyojak.txt\",'r',encoding=\"utf-8\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_rank import tokenizer as text_rank_tokenizer\n",
    "from text_rank import ranker as text_rank_ranker\n",
    "def get_summary_from_text(text,force_use_purnabiram_model=False):\n",
    "    global stop_words, word_endings, kriyapads, samyojaks\n",
    "    # \n",
    "    # Reading text files (sample text file, word endings file and stopwords file)\n",
    "    #\n",
    "    # text = open(file_path,'r',encoding=\"utf-8\").read()\n",
    "    #\n",
    "    print(f\"Input Text: \\n{text}\")\n",
    "    \n",
    "    is_complete_sentence = True\n",
    "    # if \"।\" not in text:\n",
    "    purnabiram_count = text.count(\"।\") \n",
    "    if not force_use_purnabiram_model:\n",
    "        if purnabiram_count*100 < len(text):\n",
    "            is_complete_sentence = False\n",
    "    else:\n",
    "        is_complete_sentence = False\n",
    "    # print(is_complete_sentence)   \n",
    "\n",
    "    valid_characters = text_rank_tokenizer.get_valid_chars()\n",
    "    # print(stop_words.split(\"\\n\"))\n",
    "    # print(text)\n",
    "    #\n",
    "    # Remove useless characters from the sentence \n",
    "    # \n",
    "      \n",
    "    if not is_complete_sentence:\n",
    "        text = text_rank_tokenizer.add_purnabiram(text,kriyapads,samyojaks)\n",
    "    print(f\"Sentence after adding purnabirams: \\n{text}\")  \n",
    "    \n",
    "    #\n",
    "    # Split the sentence into array of words and patagraph in its array. (as Array of Array of the words)\n",
    "    #\n",
    "    sentences = text_rank_tokenizer.get_sentences_as_arr(text)\n",
    "    # print(sentences)\n",
    "\n",
    "    text = text_rank_tokenizer.remove_useless_characters(text,valid_characters)\n",
    "\n",
    "\n",
    "    sentences = text_rank_tokenizer.remove_repeating_sentences(sentences)\n",
    "    \n",
    "    if len(sentences) == 0:\n",
    "        return \"It is not a valid text. Please try again with a valid text.\"\n",
    "    elif len(sentences) == 1:\n",
    "        return sentences\n",
    "    \n",
    "    # print(sentences)\n",
    "    words_arr = text_rank_tokenizer.get_words_as_arr(sentences)    \n",
    "    #\n",
    "    # Remove the stop words from the array\n",
    "    #\n",
    "    words_arr = text_rank_tokenizer.remove_stop_words_and_filter_word_arr(words_arr,word_endings, stop_words)\n",
    "    # print(words_arr)\n",
    "    \n",
    "    #\n",
    "    # remove empty sentences and lone word sentences and update sentences accordingly\n",
    "    #    \n",
    "    sentences, words_arr = text_rank_tokenizer.remove_empty_sentences(sentences, words_arr)\n",
    "    #\n",
    "    # Tokenize the words and sentences into numbers\n",
    "    # \n",
    "    tokens, token_dict = text_rank_tokenizer.tokenize(words_arr)\n",
    "    # \n",
    "    # Create a association matrix\n",
    "    # \n",
    "    association_matrix, counter_vector = text_rank_ranker.create_association_matrix(tokens,No_of_unique_chars= len(token_dict))\n",
    "    # \n",
    "    # Calculate influence of each word on the paragraph\n",
    "    # \n",
    "    word_influence_vector = text_rank_ranker.calculate_word_ranks(association_matrix, counter_vector)\n",
    "    # \n",
    "    # Based in the word importance ranking, calculate teh sentence importance ranking.\n",
    "    # \n",
    "    sentence_influence = text_rank_ranker.calculate_sentence_influence(tokens,word_influence_vector)\n",
    "    \n",
    "    # \n",
    "    # Get first n sentences from the given text as summarized text.\n",
    "    # \n",
    "    \n",
    "    # print(sentence_influence)\n",
    "    summary_sentences = text_rank_ranker.get_n_influencial_sentence(sentences,sentence_influence,n=np.ceil(len(sentences)*0.33))\n",
    "\n",
    "    #\n",
    "    # Combine all sentences as a single paragraph\n",
    "    #\n",
    "    summarized_text = text_rank_ranker.get_summarized_text(summary_sentences)\n",
    "    \n",
    "    print(f\"generated summary: \\n{summarized_text}\")\n",
    "    \n",
    "    # with open(outputfile, 'w',encoding=\"utf-8\") as f:\n",
    "    #     f.write(summarized_text)\n",
    "    return summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: \n",
      " तीन नगरपालिकालाई समेटेर भेरी किनारमा बन्न थालेको आधुनिक नमुना सहरको काम तीव्र गतिमा अघि बढेको छ । भेरीगंगा, गुर्भाकोट र लेकबेंसी नगरपालिकामा बन्न थालेको भेरीगंगा उपत्यका नमुना आधुनिक सहर निर्माण हुन लागेको हो । यसले नदी वारि र पारिको ४ सय ६० वर्ग किलोमिटर क्षेत्रलाई समेट्नेछ ।\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'valid_chars.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_summary_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 22\u001b[0m, in \u001b[0;36mget_summary_from_text\u001b[1;34m(text, force_use_purnabiram_model)\u001b[0m\n\u001b[0;32m     19\u001b[0m     is_complete_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# print(is_complete_sentence)   \u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m valid_characters \u001b[38;5;241m=\u001b[39m \u001b[43mtext_rank_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_valid_chars\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# print(stop_words.split(\"\\n\"))\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# print(text)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Remove useless characters from the sentence \u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_complete_sentence:\n",
      "File \u001b[1;32md:\\MajorProject\\ASRS_working\\nepali-asrs\\text_rank\\tokenizer.py:10\u001b[0m, in \u001b[0;36mget_valid_chars\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_valid_chars\u001b[39m():\n\u001b[0;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''-> valid_characters(list)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     valid_characters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid_chars.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m valid_characters\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'valid_chars.json'"
     ]
    }
   ],
   "source": [
    "get_summary_from_text(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio2text_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
